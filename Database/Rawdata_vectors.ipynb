{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From JSON to VECTORS\n",
    "\n",
    "WARNING:\n",
    "This notebook has lots of dependencies:\n",
    "\n",
    "pandas\n",
    "numpy\n",
    "nltk (plus loaded libraries from within nltk)\n",
    "sklearn\n",
    "scipy\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "1. We connect data from different sources into 1 DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We connect data from different sources into 1 DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_df = pd.read_json(\"DATA/cnn_west_virginia.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN_df = pd.read_json(\"DATA/foxnews_west_virginia.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_df = pd.read_json(\"DATA/nyt_west_virginia.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex for text cleaning\n",
    "import re\n",
    "\n",
    "#NLP library\n",
    "import nltk\n",
    "\n",
    "#Helper for creating regex \n",
    "import string\n",
    "\n",
    "# Lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning.\n",
    "# Lemmatisation depends on correctly identifying the intended part of speech and meaning\n",
    "#of a word in a sentence, as well as within the larger context surrounding that sentence\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl=WordNetLemmatizer()\n",
    "\n",
    "#pattern.en module contains a fast part-of-speech tagger for English (CLiPS)\n",
    "#from pattern.en import tag\n",
    "\n",
    "#WordNet is a lexical database for the English language.[1] It groups English words into sets of synonyms called synsets,#\n",
    "#provides short definitions and usage examples, and records a number of relations among these synonym sets or their members. \n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To delete stop words from the text\n",
    "from nltk.corpus import stopwords\n",
    "stopword_list=stopwords.words(\"english\")\n",
    "\n",
    "#Add additional stop words\n",
    "stopword_list.extend(['www','mail','edu','athttps'])\n",
    "\n",
    "#For tokenizing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#remove special characters, this is recommended\n",
    "remove_characters=re.compile('[^a-zA-Z ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "#    text = text.decode('utf-8')\n",
    "    text=text.strip()\n",
    "    filtered_sentence=re.sub(remove_characters, r' ', text)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "#pos_tagged_text is lower case and has WordNet tags, ready to lemmatize    \n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word #if word has a tag lemmatize it and add to the list, otherwise just add the word                    \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "#Converts Penn Treebank POS tags to WordNet tags    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "    #Use pattern library tagging functions (Penn Treebank syntax)\n",
    "    tagged_text = tag(text)# Result: list of tuples for each sentence\n",
    "    #In order to use lemmatizer we need to change POS tags to WordNet tags and make all words lowercase\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function removes stopwords\n",
    "def remove_stopwords(text):\n",
    "    tokens=tokenize_text(text)\n",
    "    filtered_tokens=[token for token in tokens if token not in stopword_list]\n",
    "    filtered_text=\" \".join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This fucntion tokenize words in a sentence\n",
    "def tokenize_text(text):\n",
    "    text = text.decode('utf-8')\n",
    "    tokens=nltk.word_tokenize(text)\n",
    "    tokens=[token.strip() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_abstract(abstracts):\n",
    "    normalized_abstracts=[]\n",
    "    for abstract in abstracts:\n",
    "        normalized_abstract=[]\n",
    "        #First clean data from any special characters\n",
    "        text=remove_special_characters(abstract)\n",
    "        #Split abstract into sentences\n",
    "        sentences=sent_tokenize(text)\n",
    "        for text in sentences:\n",
    "            text=lemmatize_text(text)\n",
    "            text=remove_stopwords(text)\n",
    "            normalized_abstract.append(text)\n",
    "        normalized_abstract_string=\" \".join(normalized_abstract)\n",
    "        normalized_abstracts.append(normalized_abstract_string)\n",
    "    return normalized_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(abstracts, feature_type='frequency',\n",
    "                         ngram_range=(1, 1), min_df=0.00, max_df=1.0):\n",
    "\n",
    "    feature_type = feature_type.lower().strip()  \n",
    "    \n",
    "    if feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df,\n",
    "                                     max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, \n",
    "                                     ngram_range=ngram_range,use_idf=True)\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values:'frequency', 'tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(abstracts).astype(float)\n",
    "\n",
    "    \n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=CNN_df.articles_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-be6aecc036b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 1: NORMALIZE YOUR DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnormalized_articles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize_abstract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-85b87972e5a3>\u001b[0m in \u001b[0;36mnormalize_abstract\u001b[0;34m(abstracts)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlemmatize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mnormalized_abstract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-69094283e242>\u001b[0m in \u001b[0;36mlemmatize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#pos_tagged_text is lower case and has WordNet tags, ready to lemmatize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpos_tagged_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n\u001b[1;32m      5\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m \u001b[0;31m#if word has a tag lemmatize it and add to the list, otherwise just add the word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-c9bac4bd203e>\u001b[0m in \u001b[0;36mpos_tag_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#Use pattern library tagging functions (Penn Treebank syntax)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtagged_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# Result: list of tuples for each sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m#In order to use lemmatizer we need to change POS tags to WordNet tags and make all words lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tag' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: NORMALIZE YOUR DATA\n",
    "normalized_articles=normalize_abstract(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: EXTRACT FEATURES\n",
    "tfidf_vectorizer, tfidf_matrix=build_feature_matrix(articles, feature_type=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the names of the features in the features matrix, so you are aware of what is happening\n",
    "feature_names=tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "#Calculate the adjacency matrix\n",
    "adj_matrix=cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(CNN_df)):\n",
    "    CNN_df[\"articles_text\"][i] = ' '.join(CNN_df[\"articles_text\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.27486731, 0.34715059, 0.        , 0.24461659,\n",
       "       0.37151796, 0.30174092, 0.31100895, 0.36484546, 0.2595982 ,\n",
       "       0.30856416, 0.35022805, 0.30135703, 0.27504421, 0.        ,\n",
       "       0.29009635, 0.30668694, 0.45320878, 0.40032133, 0.21575573,\n",
       "       0.33199354, 0.        , 0.        , 0.32897428, 0.36737842,\n",
       "       0.3855259 , 0.5241428 , 0.19727548, 0.        , 0.30863162,\n",
       "       0.        , 0.3263225 , 0.30820685, 0.17670712, 0.23724675,\n",
       "       0.3455495 , 0.37473999, 0.25740417, 0.34771167, 0.        ,\n",
       "       0.25751855, 0.32028396, 0.32244308, 0.33171465, 0.31548316,\n",
       "       0.38655153, 0.41494959, 0.29730837, 0.36985265, 0.35020284,\n",
       "       0.31838694, 0.2620655 , 0.26134971, 0.31023315, 0.        ,\n",
       "       0.35956499, 0.29296872, 0.37052622, 0.28860029, 0.30090425,\n",
       "       0.        , 0.39314261, 0.30765935, 0.28937816, 0.29965078,\n",
       "       0.        , 0.24816642, 0.29452681, 0.10313065, 0.34946721,\n",
       "       0.32523878, 0.3819188 , 0.3565418 , 0.1460513 , 0.41996349,\n",
       "       0.34138263, 0.28549716, 0.33109862, 0.2841791 , 0.30622961,\n",
       "       0.1739384 , 0.2248972 , 0.37404176, 0.26845217, 0.30181731,\n",
       "       0.25770191, 0.30199318, 0.        , 0.        , 0.35638729,\n",
       "       0.24445791, 0.29601754, 0.42611937, 0.34080232, 0.3331191 ,\n",
       "       0.31166106, 0.32867926, 0.39072482, 0.40738381, 0.25282225,\n",
       "       0.29010032, 0.30585375, 0.31255004, 0.36676786, 0.3977538 ,\n",
       "       0.32925176, 0.36611836])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to analyze rows of the matrix we need to change it to dense\n",
    "from scipy.sparse import csr_matrix\n",
    "matrix_dense=tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'map' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-e20483a3c749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfeature_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimportant_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfeature_lists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_tuples\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0monly_tools\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feature_list\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0monly_tools\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feature_scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'map' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for i in range(len(articles)):\n",
    "    matrix_dense_row=matrix_dense[i]\n",
    "    A = np.squeeze(np.asarray(matrix_dense_row))\n",
    "    important_features=[(d, x) for d, x in enumerate(A) if x > 0.10]\n",
    "    feature_tuples=zip(*important_features)\n",
    "    feature_lists=map(list,feature_tuples )\n",
    "    only_tools[\"feature_list\"][i]=feature_lists[0]\n",
    "    only_tools[\"feature_scores\"][i]=feature_lists[1]\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
